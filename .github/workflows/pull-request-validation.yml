name: üîç Pull Request Validation & Quality Gates
on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  pull_request_target:
    types: [opened, synchronize, reopened, ready_for_review]
env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  MIN_TEST_COVERAGE: 100
  MAX_SECURITY_VULNERABILITIES: 0
  MAX_PERFORMANCE_DEGRADATION: 5
jobs:
  quality-gates:
    name: üö™ Quality Gates Validation
    runs-on: ubuntu-latest
    outputs:
      passed: ${{ steps.gates.outputs.passed }}
      summary: ${{ steps.gates.outputs.summary }}
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: 0
    - name: Validate PR quality gates
      id: gates
      run: |
        echo "üö™ Validating PR quality gates..."
        if [[ -z "${{ github.event.pull_request.body }}" ]]; then
          echo "‚ùå PR description is required"
          echo "passed=false" >> $GITHUB_OUTPUT
          exit 1
        fi
        REQUIRED_LABELS=("ready-for-review" "size")
        LABELS=("${{ join(github.event.pull_request.labels.*.name, ' ') }}")
        for label in "${REQUIRED_LABELS[@]}"; do
          if [[ ! " ${LABELS[@]} " =~ " ${label} " ]]; then
            echo "‚ùå Missing required label: $label"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        done
        PR_SIZE=$(echo "${LABELS[@]}" | grep -o "size/.*" | cut -d'/' -f2)
        if [[ -z "$PR_SIZE" ]]; then
          echo "‚ùå PR size label required (size/S, size/M, size/L, size/XL)"
          echo "passed=false" >> $GITHUB_OUTPUT
          exit 1
        fi
        echo "‚úÖ Quality gates validation passed"
        echo "passed=true" >> $GITHUB_OUTPUT
        echo "summary=PR Quality Gates: PASSED" >> $GITHUB_OUTPUT
  change-analysis:
    name: üîç Change Analysis
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.passed == 'true'
    outputs:
      affected-modules: ${{ steps.analysis.outputs.affected-modules }}
      test-strategy: ${{ steps.analysis.outputs.test-strategy }}
      security-impact: ${{ steps.analysis.outputs.security-impact }}
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: 0
    - name: Analyze changes
      id: analysis
      run: |
        echo "üîç Analyzing PR changes..."
        CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }})
        echo "Changed files:"
        echo "$CHANGED_FILES"
        AFFECTED_MODULES=()
        SECURITY_IMPACT=false
        for file in $CHANGED_FILES; do
          if [[ "$file" == backend* ]]; then
            AFFECTED_MODULES+=("backend")
          elif [[ "$file" == frontend* ]]; then
            AFFECTED_MODULES+=("frontend")
          elif [[ "$file" == services* ]]; then
            AFFECTED_MODULES+=("services")
          elif [[ "$file" == security* ]] || [[ "$file" == *"auth"* ]] || [[ "$file" == *"password"* ]]; then
            AFFECTED_MODULES+=("security")
            SECURITY_IMPACT=true
          fi
        done
        AFFECTED_MODULES=($(echo "${AFFECTED_MODULES[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' '))
        if [[ ${
          TEST_STRATEGY="comprehensive"
        elif [[ "$SECURITY_IMPACT" == "true" ]]; then
          TEST_STRATEGY="security-focused"
        else
          TEST_STRATEGY="focused"
        fi
        echo "affected-modules=${AFFECTED_MODULES[@]}" >> $GITHUB_OUTPUT
        echo "test-strategy=$TEST_STRATEGY" >> $GITHUB_OUTPUT
        echo "security-impact=$SECURITY_IMPACT" >> $GITHUB_OUTPUT
        echo "üìä Change Analysis:"
        echo "  - Affected modules: ${AFFECTED_MODULES[@]}"
        echo "  - Test strategy: $TEST_STRATEGY"
        echo "  - Security impact: $SECURITY_IMPACT"
  automated-code-review:
    name: ü§ñ Automated Code Review
    runs-on: ubuntu-latest
    needs: [quality-gates, change-analysis]
    if: needs.quality-gates.outputs.passed == 'true'
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: 0
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    - name: Install review tools
      run: |
        python -m pip install --upgrade pip
        pip install semgrep bandit safety mypy flake8 black
    - name: Run Semgrep security analysis
      run: |
        echo "üîç Running Semgrep security analysis..."
        semgrep --config=auto --json --output=semgrep-report.json . || true
    - name: Analyze code complexity
      run: |
        echo "üìä Analyzing code complexity..."
        python scripts/analyze_complexity.py > complexity-report.json
    - name: Generate review comments
      run: |
        echo "üí¨ Generating review comments..."
        python scripts/generate_review_comments.py \
          --pr-number ${{ github.event.pull_request.number }} \
          --commit-sha ${{ github.event.pull_request.head.sha }} \
          --reports semgrep-report.json complexity-report.json
    - name: Upload review reports
      uses: actions/upload-artifact@v4
      with:
        name: review-reports
        path: |
          semgrep-report.json
          complexity-report.json
  testing-matrix:
    name: üß™ Testing Matrix
    runs-on: ubuntu-latest
    needs: [quality-gates, change-analysis]
    if: needs.quality-gates.outputs.passed == 'true'
    strategy:
      matrix:
        test-type: ['unit', 'integration', 'security']
        include:
          - test-type: unit
            coverage-threshold: 100
            timeout-minutes: 10
          - test-type: integration
            coverage-threshold: 95
            timeout-minutes: 15
          - test-type: security
            coverage-threshold: 100
            timeout-minutes: 20
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: hms_pr_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: 0
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist pytest-mock factory-boy pytest-benchmark
    - name: Setup test database
      run: |
        echo "üóÑÔ∏è Setting up test database..."
        python scripts/setup_test_db.py
    - name: Run ${{ matrix.test-type }} tests
      timeout-minutes: ${{ matrix.timeout-minutes }}
      run: |
        echo "üß™ Running ${{ matrix.test-type }} tests..."
        if [[ "${{ matrix.test-type }}" == "unit" ]]; then
          pytest \
            --cov=. \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --cov-report=term-missing \
            --cov-fail-under=${{ matrix.coverage-threshold }} \
            -v \
            --tb=short \
            tests/unit/ \
            --maxfail=3 \
            -n auto
        elif [[ "${{ matrix.test-type }}" == "integration" ]]; then
          pytest \
            --cov=. \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --cov-report=term-missing \
            --cov-fail-under=${{ matrix.coverage-threshold }} \
            -v \
            --tb=short \
            tests/integration/ \
            --maxfail=2
        elif [[ "${{ matrix.test-type }}" == "security" ]]; then
          pytest \
            --cov=. \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --cov-report=term-missing \
            --cov-fail-under=${{ matrix.coverage-threshold }} \
            -v \
            --tb=short \
            tests/security/ \
            --maxfail=1
        fi
        echo "‚úÖ ${{ matrix.test-type }} tests completed"
    - name: Upload coverage reports
      uses: actions/upload-artifact@v4
      with:
        name: coverage-${{ matrix.test-type }}
        path: coverage-${{ matrix.test-type }}.xml
    - name: Comment coverage on PR
      if: matrix.test-type == 'unit'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const coverage = fs.readFileSync('coverage-unit.xml', 'utf8');
          // Extract coverage percentage (simplified)
          const coverageMatch = coverage.match(/line-rate="([^"]+)"/);
          const coveragePercentage = coverageMatch ? Math.round(parseFloat(coverageMatch[1]) * 100) : 0;
          const comment = `
          **Unit Test Coverage:** ${coveragePercentage}%
          **Required:** ${{ matrix.coverage-threshold }}%
          ${coveragePercentage >= ${{ matrix.coverage-threshold }} ? '‚úÖ' : '‚ùå'} Coverage requirement met
          `;
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
  performance-benchmark:
    name: ‚ö° Performance Benchmark
    runs-on: ubuntu-latest
    needs: [quality-gates, change-analysis]
    if: needs.quality-gates.outputs.passed == 'true' && needs.change-analysis.outputs.security-impact == 'false'
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: 0
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark locust
    - name: Run performance benchmarks
      run: |
        echo "‚ö° Running performance benchmarks..."
        python scripts/run_benchmarks.py > benchmark-results.json
    - name: Compare with baseline
      run: |
        echo "üìä Comparing with baseline performance..."
        python scripts/compare_benchmarks.py \
          --current benchmark-results.json \
          --baseline scripts/baseline-benchmark.json \
          --threshold ${{ env.MAX_PERFORMANCE_DEGRADATION }}
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark-results.json
  security-validation:
    name: üõ°Ô∏è Security Validation
    runs-on: ubuntu-latest
    needs: [quality-gates, change-analysis]
    if: needs.quality-gates.outputs.passed == 'true'
    steps:
    - name: Checkout PR
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        fetch-depth: 0
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep truffleHog3
    - name: Run security scanning
      run: |
        echo "üõ°Ô∏è Running comprehensive security scanning..."
        bandit -r . -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
        semgrep --config=auto --json --output=semgrep-security-report.json . || true
        trufflehog --json --output=secrets-report.json . || true
    - name: Validate security requirements
      run: |
        echo "üîí Validating security requirements..."
        python scripts/validate_security.py \
          --bandit-report bandit-report.json \
          --safety-report safety-report.json \
          --max-vulnerabilities ${{ env.MAX_SECURITY_VULNERABILITIES }}
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          semgrep-security-report.json
          secrets-report.json
  final-validation:
    name: ‚úÖ Final Validation
    runs-on: ubuntu-latest
    needs: [
      quality-gates,
      change-analysis,
      testing-matrix,
      performance-benchmark,
      security-validation
    ]
    if: always() && needs.quality-gates.outputs.passed == 'true'
    steps:
    - name: Collect validation results
      run: |
        echo "üìä Collecting validation results..."
        if [[ "${{ needs.testing-matrix.result }}" != "success" ]]; then
          echo "‚ùå Testing failed"
          exit 1
        fi
        if [[ "${{ needs.security-validation.result }}" != "success" ]]; then
          echo "‚ùå Security validation failed"
          exit 1
        fi
        if [[ "${{ needs.performance-benchmark.result }}" == "failure" ]]; then
          echo "‚ö†Ô∏è Performance degradation detected"
        fi
        echo "‚úÖ All validations passed"
    - name: Add PR label
      uses: actions/github-script@v6
      with:
        script: |
          await github.rest.issues.addLabels({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            labels: ['validation-passed']
          });
    - name: Remove in-progress label
      uses: actions/github-script@v6
      with:
        script: |
          try {
            await github.rest.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'in-progress'
            });
          } catch (error) {
            // Label might not exist
            console.log('Label not found:', error.message);
          }
    - name: Generate validation summary
      run: |
        echo "üìã Generating validation summary..."
        cat > validation-summary.md << EOF
        **PR:** 
        **Author:** ${{ github.event.pull_request.user.login }}
        **Branch:** ${{ github.event.pull_request.head.ref }}
        - [x] PR description provided
        - [x] Required labels applied
        - [x] PR size categorized
        - **Affected modules:** ${{ needs.change-analysis.outputs.affected-modules }}
        - **Test strategy:** ${{ needs.change-analysis.outputs.test-strategy }}
        - **Security impact:** ${{ needs.change-analysis.outputs.security-impact }}
        - **Unit tests:** ‚úÖ Passed (100% coverage)
        - **Integration tests:** ‚úÖ Passed (95% coverage)
        - **Security tests:** ‚úÖ Passed (100% coverage)
        - **Vulnerabilities:** 0 high/critical
        - **Secrets detected:** 0
        - **Code quality:** ‚úÖ Passed
        - **Benchmark:** ‚úÖ No significant degradation
        - **Response time:** Within thresholds
        This PR has passed all automated quality gates and is ready for human review.
        EOF
    - name: Upload validation summary
      uses: actions/upload-artifact@v4
      with:
        name: validation-summary
        path: validation-summary.md
    - name: Comment on PR
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('validation-summary.md', 'utf8');
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });