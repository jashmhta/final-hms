# Enterprise-Grade Monitoring and Observability Stack
# Author: Microservices Scaling Architect
# Purpose: Complete monitoring with Prometheus, Grafana, distributed tracing, and logging

# ==================== PROMETHEUS CONFIGURATION ====================
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'hms-production'
        region: 'us-east-1'

    # Alertmanager configuration
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093

    # HMS-specific recording rules
    recording_rules:
      - name: hms_services:health_score
        expr: (up{job=~"hms-.*"} == 1) * 100
      - name: hms_services:error_rate
        expr: (rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])) * 100
      - name: hms_services:latency_p95
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
      - name: hms_database:connection_usage
        expr: (pg_stat_database_numbackends / pg_settings_max_connections) * 100
      - name: hms_kafka:consumer_lag
        expr: kafka_consumer_lag{topic=~"hms-.*"}

    # HMS-specific alerting rules
    rules:
      - alert: HMS_ServiceDown
        expr: up{job=~"hms-.*"} == 0
        for: 1m
        labels:
          severity: critical
          team: operations
        annotations:
          summary: "HMS service {{ $labels.instance }} is down"
          description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute"

      - alert: HMS_HighErrorRate
        expr: hms_services:error_rate > 5
        for: 5m
        labels:
          severity: warning
          team: operations
        annotations:
          summary: "High error rate in HMS service {{ $labels.job }}"
          description: "Error rate of {{ $value }}% detected in service {{ $labels.job }}"

      - alert: HMS_HighLatency
        expr: hms_services:latency_p95 > 2
        for: 5m
        labels:
          severity: warning
          team: performance
        annotations:
          summary: "High latency in HMS service {{ $labels.job }}"
          description: "95th percentile latency of {{ $value }}s detected in service {{ $labels.job }}"

      - alert: HMS_DatabaseConnectionsHigh
        expr: hms_database:connection_usage > 80
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "High database connection usage"
          description: "Database connection usage is {{ $value }}%, above 80% threshold"

      - alert: HMS_KafkaConsumerLag
        expr: hms_kafka:consumer_lag > 1000
        for: 5m
        labels:
          severity: warning
          team: messaging
        annotations:
          summary: "High Kafka consumer lag"
          description: "Consumer lag of {{ $value }} messages detected for topic {{ $labels.topic }}"

      - alert: HMS_DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value }}% disk space remaining on {{ $labels.instance }}"

      - alert: HMS_MemoryUsageHigh
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"

    # Scrape configurations
    scrape_configs:
      # Kubernetes service discovery
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Kubernetes nodes
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics

      # Kubernetes pods
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

      # HMS Services
      - job_name: 'hms-backend'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ['hms-production']
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: job
            regex: (hms-backend|patient-service|appointment-service|pharmacy-service|lab-service|billing-service|er-alerts)

      - job_name: 'hms-database'
        static_configs:
          - targets: ['postgres-exporter:9187']
        metrics_path: /metrics

      - job_name: 'hms-redis'
        static_configs:
          - targets: ['redis-exporter:9121']
        metrics_path: /metrics

      - job_name: 'hms-kafka'
        static_configs:
          - targets: ['kafka-broker:9404']
        metrics_path: /metrics

      - job_name: 'hms-kong'
        static_configs:
          - targets: ['kong-proxy:9542']
        metrics_path: /metrics

      # Node exporters
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

---
# ==================== PROMETHEUS DEPLOYMENT ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
  labels:
    app: prometheus
    component: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
        component: monitoring
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      serviceAccountName: prometheus
      containers:
        - name: prometheus
          image: prom/prometheus:v2.45.0
          args:
            - '--storage.tsdb.retention.time=30d'
            - '--storage.tsdb.retention.size=50GB'
            - '--web.enable-lifecycle'
            - '--web.enable-admin-api'
            - '--config.file=/etc/prometheus/prometheus.yml'
            - '--storage.tsdb.path=/prometheus'
            - '--web.console.libraries=/usr/share/prometheus/console_libraries'
            - '--web.console.templates=/usr/share/prometheus/consoles'
            - '--web.enable-remote-write-receiver'
          ports:
            - containerPort: 9090
              name: web
          volumeMounts:
            - name: prometheus-config-volume
              mountPath: /etc/prometheus
            - name: prometheus-storage-volume
              mountPath: /prometheus
          resources:
            requests:
              memory: "2Gi"
              cpu: "1000m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: prometheus-config-volume
          configMap:
            name: prometheus-config
        - name: prometheus-storage-volume
          persistentVolumeClaim:
            claimName: prometheus-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
  labels:
    app: prometheus
    service: monitoring
spec:
  type: ClusterIP
  selector:
    app: prometheus
  ports:
    - name: web
      port: 9090
      targetPort: 9090

---
# ==================== GRAFANA CONFIGURATION ====================
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        orgId: 1
        url: http://prometheus:9090
        basicAuth: false
        isDefault: true
        version: 1
        editable: false
        jsonData:
          timeInterval: 15s
          queryTimeout: 60s
          httpMethod: POST
          sigV4Auth: false
      - name: Loki
        type: loki
        access: proxy
        orgId: 1
        url: http://loki:3100
        basicAuth: false
        isDefault: false
        version: 1
        editable: false
      - name: Jaeger
        type: jaeger
        access: proxy
        orgId: 1
        url: http://jaeger-query:16686
        basicAuth: false
        isDefault: false
        version: 1
        editable: false
      - name: PostgreSQL
        type: postgres
        access: proxy
        orgId: 1
        url: postgres-exporter:5432
        database: hms_production
        user: postgres
        secureJsonData:
          password: ${POSTGRES_PASSWORD}
        isDefault: false
        version: 1
        editable: true
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        options:
          path: /var/lib/grafana/dashboards
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-providers
  namespace: monitoring
data:
  hms-overview-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "HMS System Overview",
        "tags": ["hms", "overview"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "System Health Score",
            "type": "stat",
            "targets": [
              {
                "expr": "avg(hms_services:health_score)",
                "legendFormat": "Health Score"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "min": 0,
                "max": 100,
                "thresholds": {
                  "steps": [
                    {"color": "red", "value": 0},
                    {"color": "yellow", "value": 70},
                    {"color": "green", "value": 90}
                  ]
                }
              }
            },
            "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total[5m])",
                "legendFormat": "{{ job }}"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 6, "y": 0}
          },
          {
            "id": 3,
            "title": "Error Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "hms_services:error_rate",
                "legendFormat": "{{ job }}"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 4}
          },
          {
            "id": 4,
            "title": "Response Time P95",
            "type": "graph",
            "targets": [
              {
                "expr": "hms_services:latency_p95",
                "legendFormat": "{{ job }}"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 4}
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
    component: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
        component: monitoring
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:10.2.0
          env:
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: grafana-secrets
                  key: admin-password
            - name: GF_USERS_ALLOW_SIGN_UP
              value: "false"
            - name: GF_INSTALL_PLUGINS
              value: "grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel,grafana-piechart-panel"
          ports:
            - containerPort: 3000
              name: http
          volumeMounts:
            - name: grafana-datasources
              mountPath: /etc/grafana/provisioning/datasources
            - name: grafana-dashboards
              mountPath: /etc/grafana/provisioning/dashboards
            - name: grafana-dashboard-providers
              mountPath: /var/lib/grafana/dashboards
            - name: grafana-storage
              mountPath: /var/lib/grafana
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: grafana-datasources
          configMap:
            name: grafana-datasources
        - name: grafana-dashboards
          configMap:
            name: grafana-dashboards
        - name: grafana-dashboard-providers
          configMap:
            name: grafana-dashboard-providers
        - name: grafana-storage
          persistentVolumeClaim:
            claimName: grafana-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
    service: monitoring
spec:
  type: LoadBalancer
  selector:
    app: grafana
  ports:
    - name: http
      port: 80
      targetPort: 3000

---
# ==================== ALERTMANAGER CONFIGURATION ====================
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@hms.enterprise'
      smtp_auth_username: 'alerts@hms.enterprise'
      smtp_auth_password: '${SMTP_PASSWORD}'

    templates:
      - '/etc/alertmanager/templates/*.tmpl'

    route:
      group_by: ['alertname', 'severity', 'cluster']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'default'
      routes:
        - match:
            severity: critical
          receiver: 'critical-alerts'
        - match:
            severity: warning
          receiver: 'warning-alerts'
        - match:
            service: er-alerts
          receiver: 'emergency-alerts'

    receivers:
      - name: 'default'
        email_configs:
          - to: 'operations@hms.enterprise'
            subject: 'HMS Alert: {{ .GroupLabels.alertname }}'
            body: |
              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Labels: {{ .Labels }}
              Starts at: {{ .StartsAt }}
              {{ end }}

      - name: 'critical-alerts'
        email_configs:
          - to: 'emergency@hms.enterprise,operations@hms.enterprise'
            subject: 'üö® CRITICAL HMS Alert: {{ .GroupLabels.alertname }}'
            body: |
              üö® CRITICAL ALERT üö®

              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Service: {{ .Labels.job }}
              Instance: {{ .Labels.instance }}
              Severity: {{ .Labels.severity }}
              Starts at: {{ .StartsAt }}
              {{ end }}

              Immediate action required!

      - name: 'warning-alerts'
        email_configs:
          - to: 'operations@hms.enterprise'
            subject: '‚ö†Ô∏è HMS Warning: {{ .GroupLabels.alertname }}'
            body: |
              ‚ö†Ô∏è WARNING ALERT ‚ö†Ô∏è

              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Service: {{ .Labels.job }}
              Instance: {{ .Labels.instance }}
              Severity: {{ .Labels.severity }}
              Starts at: {{ .StartsAt }}
              {{ end }}

      - name: 'emergency-alerts'
        webhook_configs:
          - url: 'http://webhook-service:8080/emergency'
            send_resolved: true
        email_configs:
          - to: 'emergency-team@hms.enterprise,oncall@hms.enterprise'
            subject: 'üÜò EMERGENCY HMS Alert: {{ .GroupLabels.alertname }}'
            body: |
              üÜò EMERGENCY ALERT üÜò

              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Service: {{ .Labels.job }}
              Instance: {{ .Labels.instance }}
              Severity: {{ .Labels.severity }}
              Starts at: {{ .StartsAt }}
              {{ end }}

              IMMEDIATE RESPONSE REQUIRED!
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
    component: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
        component: monitoring
    spec:
      containers:
        - name: alertmanager
          image: prom/alertmanager:v0.26.0
          args:
            - '--config.file=/etc/alertmanager/alertmanager.yml'
            - '--storage.path=/alertmanager'
            - '--web.external-url=http://alertmanager.monitoring.svc.cluster.local:9093'
          ports:
            - containerPort: 9093
              name: web
          volumeMounts:
            - name: alertmanager-config
              mountPath: /etc/alertmanager
            - name: alertmanager-storage
              mountPath: /alertmanager
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9093
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9093
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: alertmanager-config
          configMap:
            name: alertmanager-config
        - name: alertmanager-storage
          persistentVolumeClaim:
            claimName: alertmanager-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
    service: monitoring
spec:
  type: ClusterIP
  selector:
    app: alertmanager
  ports:
    - name: web
      port: 9093
      targetPort: 9093

---
# ==================== JAEGER TRACING CONFIGURATION =================---
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-config
  namespace: monitoring
data:
  collector.yaml: |
    collector:
      zipkin:
        host-port: :9411
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    query:
      base-path: /
    storage:
      type: elasticsearch
      elasticsearch:
        server-urls: http://elasticsearch:9200
        index-prefix: jaeger
        username: elastic
        password: ${ELASTIC_PASSWORD}
    esIndexCleaner:
      enabled: true
      numberOfDays: 7
      schedule: 0 0 * * *
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-collector
  namespace: monitoring
  labels:
    app: jaeger-collector
    component: tracing
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jaeger-collector
  template:
    metadata:
      labels:
        app: jaeger-collector
        component: tracing
    spec:
      containers:
        - name: jaeger-collector
          image: jaegertracing/jaeger-collector:1.49
          env:
            - name: COLLECTOR_ZIPKIN_HOST_PORT
              value: :9411
            - name: COLLECTOR_OTLP_ENABLED
              value: "true"
            - name: COLLECTOR_OTLP_GRPC_HOST_PORT
              value: :4317
            - name: COLLECTOR_OTLP_HTTP_HOST_PORT
              value: :4318
          ports:
            - containerPort: 9411
              name: zipkin
            - containerPort: 4317
              name: otlp-grpc
            - containerPort: 4318
              name: otlp-http
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  namespace: monitoring
  labels:
    app: jaeger-collector
    service: tracing
spec:
  type: ClusterIP
  selector:
    app: jaeger-collector
  ports:
    - name: zipkin
      port: 9411
      targetPort: 9411
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-query
  namespace: monitoring
  labels:
    app: jaeger-query
    component: tracing
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jaeger-query
  template:
    metadata:
      labels:
        app: jaeger-query
        component: tracing
    spec:
      containers:
        - name: jaeger-query
          image: jaegertracing/jaeger-query:1.49
          env:
            - name: QUERY_BASE_PATH
              value: /
            - name: JAEGER_STORAGE
              value: elasticsearch
            - name: ES_SERVER_URLS
              value: http://elasticsearch:9200
            - name: ES_INDEX_PREFIX
              value: jaeger
            - name: ES_USERNAME
              value: elastic
            - name: ES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elastic-secrets
                  key: password
          ports:
            - containerPort: 16686
              name: query
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  namespace: monitoring
  labels:
    app: jaeger-query
    service: tracing
spec:
  type: ClusterIP
  selector:
    app: jaeger-query
  ports:
    - name: query
      port: 16686
      targetPort: 16686

---
# ==================== ELASTICSEARCH FOR LOGGING ====================
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-config
  namespace: monitoring
data:
  elasticsearch.yml: |
    cluster.name: hms-logging
    network.host: 0.0.0.0
    discovery.type: single-node
    xpack.security.enabled: true
    xpack.security.http.ssl.enabled: false
    xpack.monitoring.collection.enabled: true
    indices.query.bool.max_clause_count: 4096
    action.auto_create_index: true
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: monitoring
  labels:
    app: elasticsearch
    component: logging
spec:
  serviceName: elasticsearch
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
        component: logging
    spec:
      containers:
        - name: elasticsearch
          image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
          env:
            - name: discovery.type
              value: single-node
            - name: ES_JAVA_OPTS
              value: "-Xms1g -Xmx1g"
            - name: ELASTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elastic-secrets
                  key: password
          ports:
            - containerPort: 9200
              name: http
            - containerPort: 9300
              name: transport
          volumeMounts:
            - name: elasticsearch-data
              mountPath: /usr/share/elasticsearch/data
            - name: elasticsearch-config
              mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
              subPath: elasticsearch.yml
          resources:
            requests:
              memory: "2Gi"
              cpu: "1000m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
          livenessProbe:
            tcpSocket:
              port: 9200
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /_cluster/health
              port: 9200
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: elasticsearch-config
          configMap:
            name: elasticsearch-config
  volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: monitoring
  labels:
    app: elasticsearch
    service: logging
spec:
  type: ClusterIP
  selector:
    app: elasticsearch
  ports:
    - name: http
      port: 9200
      targetPort: 9200
    - name: transport
      port: 9300
      targetPort: 9300

---
# ==================== KIBANA DASHBOARD ====================
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana-config
  namespace: monitoring
data:
  kibana.yml: |
    server.host: 0.0.0.0
    elasticsearch.hosts: ["http://elasticsearch:9200"]
    elasticsearch.username: elastic
    elasticsearch.password: ${ELASTIC_PASSWORD}
    xpack.security.enabled: true
    xpack.monitoring.ui.container.elasticsearch.enabled: true
    server.maxPayloadBytes: 1048576
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: monitoring
  labels:
    app: kibana
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
        component: logging
    spec:
      containers:
        - name: kibana
          image: docker.elastic.co/kibana/kibana:8.10.0
          env:
            - name: ELASTICSEARCH_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elastic-secrets
                  key: password
          ports:
            - containerPort: 5601
              name: http
          volumeMounts:
            - name: kibana-config
              mountPath: /usr/share/kibana/config/kibana.yml
              subPath: kibana.yml
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /api/status
              port: 5601
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /api/status
              port: 5601
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: kibana-config
          configMap:
            name: kibana-config
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: monitoring
  labels:
    app: kibana
    service: logging
spec:
  type: LoadBalancer
  selector:
    app: kibana
  ports:
    - name: http
      port: 80
      targetPort: 5601

---
# ==================== NODE EXPORTER ====================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
    component: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
        component: monitoring
    spec:
      hostNetwork: true
      hostPID: true
      containers:
        - name: node-exporter
          image: prom/node-exporter:v1.6.1
          args:
            - '--path.sysfs=/host/sys'
            - '--path.rootfs=/rootfs'
            - '--no-collector.wifi'
            - '--no-collector.hwmon'
            - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)'
          ports:
            - containerPort: 9100
              name: metrics
          volumeMounts:
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: rootfs
              mountPath: /rootfs
              readOnly: true
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
      volumes:
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs
          hostPath:
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
    service: monitoring
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app: node-exporter
  ports:
    - name: metrics
      port: 9100
      targetPort: 9100

---
# ==================== POSTGRES EXPORTER ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-exporter
  namespace: monitoring
  labels:
    app: postgres-exporter
    component: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-exporter
  template:
    metadata:
      labels:
        app: postgres-exporter
        component: monitoring
    spec:
      containers:
        - name: postgres-exporter
          image: prometheuscommunity/postgres-exporter:v0.11.1
          env:
            - name: DATA_SOURCE_NAME
              value: "postgresql://postgres:$(POSTGRES_PASSWORD)@postgres:5432/hms_production?sslmode=disable"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secrets
                  key: password
          ports:
            - containerPort: 9187
              name: metrics
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-exporter
  namespace: monitoring
  labels:
    app: postgres-exporter
    service: monitoring
spec:
  type: ClusterIP
  selector:
    app: postgres-exporter
  ports:
    - name: metrics
      port: 9187
      targetPort: 9187

---
# ==================== REDIS EXPORTER ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-exporter
  namespace: monitoring
  labels:
    app: redis-exporter
    component: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-exporter
  template:
    metadata:
      labels:
        app: redis-exporter
        component: monitoring
    spec:
      containers:
        - name: redis-exporter
          image: oliver006/redis_exporter:v1.50.0
          args:
            - '--redis.addr=redis://redis-cluster:6379'
            - '--redis.password=$(REDIS_PASSWORD)'
            - '--check-keys=hms_*'
            - '--check-single-keys=hms_stats'
          env:
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: redis-secrets
                  key: password
          ports:
            - containerPort: 9121
              name: metrics
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: redis-exporter
  namespace: monitoring
  labels:
    app: redis-exporter
    service: monitoring
spec:
  type: ClusterIP
  selector:
    app: redis-exporter
  ports:
    - name: metrics
      port: 9121
      targetPort: 9121

---
# ==================== PERSISTENT VOLUME CLAIMS ====================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 10Gi

---
# ==================== SECRETS ====================
apiVersion: v1
kind: Secret
metadata:
  name: grafana-secrets
  namespace: monitoring
type: Opaque
data:
  admin-password: YWRtaW4xMjM=  # admin123
---
apiVersion: v1
kind: Secret
metadata:
  name: elastic-secrets
  namespace: monitoring
type: Opaque
data:
  password: ZWxhc3RpYzEyMw==  # elastic123
---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secrets
  namespace: monitoring
type: Opaque
data:
  password: cG9zdGdyZXMTIz  # postgres123
---
apiVersion: v1
kind: Secret
metadata:
  name: redis-secrets
  namespace: monitoring
type: Opaque
data:
  password: cmVkaXNfMTIz  # redis_123

---
# ==================== RBAC CONFIGURATION ====================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
      - configmaps
    verbs: ["get"]
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: monitoring

---
# ==================== MONITORING INGRESS ====================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: monitoring-ingress
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
    - hosts:
        - "grafana.hms.enterprise"
        - "kibana.hms.enterprise"
      secretName: monitoring-tls-secret
  rules:
    - host: "grafana.hms.enterprise"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 80
    - host: "kibana.hms.enterprise"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kibana
                port:
                  number: 80