# HMS Enterprise-Grade System Architecture

## üèóÔ∏è Architecture Overview

The HMS Enterprise-Grade system is built on a modern, microservices architecture designed specifically for healthcare applications. This document provides a comprehensive overview of the system's technical architecture, design patterns, and implementation details.

### Architecture Principles

#### Healthcare-Specific Design
- **Domain-Driven Design (DDD)**: Healthcare domain boundaries and contexts
- **Event-Driven Architecture**: Real-time healthcare event processing
- **Microservices**: Isolated services for healthcare-specific functions
- **Zero-Trust Security**: End-to-end security for PHI data
- **HIPAA Compliance**: Built-in compliance and audit capabilities

#### Technical Principles
- **Scalability**: Horizontal scaling for high availability
- **Resilience**: Fault tolerance and graceful degradation
- **Performance**: Sub-200ms response times for critical operations
- **Maintainability**: Clean architecture with clear separation of concerns
- **Observability**: Comprehensive monitoring and logging

## üéØ System Architecture

### High-Level Architecture

```mermaid
graph TB
    subgraph "Client Layer"
        A[Web Application]
        B[Mobile Applications]
        C[Desktop Clients]
        D[External Systems]
    end

    subgraph "API Gateway Layer"
        E[API Gateway<br/>Kong/Apollo]
        F[Load Balancer<br/>Nginx/HAProxy]
        G[Web Application Firewall<br/>Cloudflare/AWS WAF]
    end

    subgraph "Core Services Layer"
        H[Patient Service]
        I[Clinical Service]
        J[Billing Service]
        K[Appointment Service]
        L[Pharmacy Service]
        M[Laboratory Service]
        N[Radiology Service]
        O[Notification Service]
        P[Analytics Service]
    end

    subgraph "Support Services Layer"
        Q[Authentication Service]
        R[Audit Service]
        S[Integration Service]
        T[Backup Service]
        U[Monitoring Service]
    end

    subgraph "Data Layer"
        V[(PostgreSQL<br/>Primary Database)]
        W[(PostgreSQL<br/>Read Replicas)]
        X[(Redis<br/>Cache Layer)]
        Y[(MongoDB<br/>Document Store)]
        Z[(Elasticsearch<br/>Search Engine)]
    end

    subgraph "Infrastructure Layer"
        AA[Kubernetes Cluster]
        AB[Docker Containers]
        AC[Message Queue<br/>Apache Kafka]
        AD[Object Storage<br/>AWS S3/MinIO]
        AE[Backup Storage<br/>AWS Glacier]
    end

    A --> E
    B --> E
    C --> E
    D --> E

    E --> F
    F --> G

    G --> H
    G --> I
    G --> J
    G --> K
    G --> L
    G --> M
    G --> N
    G --> O
    G --> P

    H --> V
    I --> V
    J --> V
    K --> V
    L --> V
    M --> V
    N --> V
    O --> V
    P --> V

    H --> X
    I --> X
    J --> X
    K --> X
    L --> X
    M --> X
    N --> X
    O --> X
    P --> X

    H --> Y
    I --> Y
    L --> Y
    N --> Y

    M --> Z
    P --> Z

    Q --> AC
    R --> AC
    S --> AC
    T --> AC
    U --> AC

    H --> AA
    I --> AA
    J --> AA
    K --> AA
    L --> AA
    M --> AA
    N --> AA
    O --> AA
    P --> AA
    Q --> AA
    R --> AA
    S --> AA
    T --> AA
    U --> AA

    V --> AE
    W --> AE
    X --> AE
    Y --> AE
    Z --> AE
    AC --> AE
    AD --> AE
```

### Microservices Architecture

#### Core Healthcare Services

**Patient Service**
- **Responsibility**: Patient lifecycle management
- **Data Model**: Patient demographics, medical history, contacts
- **API Endpoints**: CRUD operations for patient data
- **Events**: patient.created, patient.updated, patient.deleted
- **Database**: PostgreSQL with patient-specific schema

**Clinical Service**
- **Responsibility**: Clinical documentation and workflows
- **Data Model**: Clinical notes, assessments, treatment plans
- **API Endpoints**: Clinical documentation, SOAP notes, templates
- **Events**: clinical.note.created, clinical.assessment.updated
- **Database**: PostgreSQL with clinical data schema

**Billing Service**
- **Responsibility**: Financial operations and insurance claims
- **Data Model**: Charges, payments, insurance claims, accounts
- **API Endpoints**: Charge entry, claim submission, payment processing
- **Events**: billing.charge.created, billing.claim.submitted
- **Database**: PostgreSQL with billing schema

**Appointment Service**
- **Responsibility**: Scheduling and resource management
- **Data Model**: Appointments, schedules, resources, calendars
- **API Endpoints**: Appointment scheduling, calendar management
- **Events**: appointment.created, appointment.rescheduled
- **Database**: PostgreSQL with scheduling schema

**Pharmacy Service**
- **Responsibility**: Medication management and prescribing
- **Data Model**: Prescriptions, medications, inventory, interactions
- **API Endpoints**: E-prescribing, drug interactions, inventory
- **Events**: prescription.created, medication.dispensed
- **Database**: PostgreSQL with pharmacy schema

**Laboratory Service**
- **Responsibility**: Laboratory test ordering and results
- **Data Model**: Lab orders, test results, specimens, panels
- **API Endpoints**: Test ordering, result reporting, quality control
- **Events**: lab.order.created, lab.result.available
- **Database**: PostgreSQL with laboratory schema

**Radiology Service**
- **Responsibility**: Imaging studies and reports
- **Data Model**: Imaging orders, DICOM images, reports
- **API Endpoints**: Imaging orders, DICOM viewing, reporting
- **Events**: imaging.order.created, imaging.study.available
- **Database**: PostgreSQL + MongoDB for DICOM metadata

#### Support Services

**Authentication Service**
- **Responsibility**: Identity and access management
- **Technology**: OAuth 2.0, OpenID Connect, JWT
- **Features**: MFA, SSO, RBAC, session management
- **Integration**: External identity providers (Auth0, Azure AD)

**Audit Service**
- **Responsibility**: Comprehensive audit logging
- **Technology**: Event streaming, immutable audit logs
- **Features**: HIPAA audit trails, compliance reporting
- **Integration**: All services for comprehensive auditing

**Integration Service**
- **Responsibility**: Third-party system integrations
- **Technology**: HL7/FHIR, REST APIs, message queues
- **Features**: EHR integration, laboratory systems, radiology systems
- **Standards**: HL7 v2/v3, FHIR R4, DICOM

**Notification Service**
- **Responsibility**: Communication and alerts
- **Technology**: Email, SMS, push notifications
- **Features**: Appointment reminders, critical alerts, notifications
- **Integration**: Twilio, SendGrid, Firebase Cloud Messaging

**Analytics Service**
- **Responsibility**: Business intelligence and reporting
- **Technology**: Data warehouse, BI tools, ML models
- **Features**: Clinical analytics, financial reporting, population health
- **Integration**: All services for data aggregation

## üóÑÔ∏è Data Architecture

### Database Strategy

#### Polyglot Persistence
- **PostgreSQL**: Primary transactional database for structured data
- **MongoDB**: Document storage for unstructured medical records
- **Redis**: High-performance caching and session storage
- **Elasticsearch**: Full-text search and analytics
- **InfluxDB**: Time-series data for monitoring and metrics

#### Database Design Principles
- **Database per Service**: Each microservice has its own database
- **Shared Nothing**: No shared databases between services
- **Event Sourcing**: Critical events stored in immutable logs
- **CQRS**: Separate read and write models for complex queries
- **Data Consistency**: Eventual consistency with sagas

### Data Models

#### Patient Data Model
```sql
CREATE TABLE patients (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    patient_id VARCHAR(20) UNIQUE NOT NULL,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    date_of_birth DATE NOT NULL,
    gender VARCHAR(1) NOT NULL CHECK (gender IN ('M', 'F', 'O')),
    contact_number VARCHAR(20) NOT NULL,
    email VARCHAR(255),
    address JSONB,
    emergency_contact JSONB,
    blood_type VARCHAR(3),
    allergies TEXT[],
    medications JSONB[],
    medical_conditions JSONB[],
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB
);

-- Indexes for performance
CREATE INDEX idx_patients_name ON patients (last_name, first_name);
CREATE INDEX idx_patients_dob ON patients (date_of_birth);
CREATE INDEX idx_patients_active ON patients (is_active);
CREATE INDEX idx_patients_created ON patients (created_at);
```

#### Clinical Data Model
```sql
CREATE TABLE clinical_encounters (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    patient_id UUID NOT NULL REFERENCES patients(id),
    provider_id UUID NOT NULL,
    encounter_type VARCHAR(50) NOT NULL,
    encounter_date TIMESTAMP WITH TIME ZONE NOT NULL,
    location VARCHAR(100),
    chief_complaint TEXT,
    history_of_present_illness TEXT,
    review_of_systems JSONB,
    physical_examination JSONB,
    assessment TEXT,
    plan TEXT,
    vital_signs JSONB,
    diagnoses JSONB[],
    procedures JSONB[],
    medications JSONB[],
    follow_up_required BOOLEAN DEFAULT false,
    follow_up_date DATE,
    status VARCHAR(20) DEFAULT 'completed',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB
);

-- Indexes for clinical queries
CREATE INDEX idx_encounters_patient ON clinical_encounters(patient_id);
CREATE INDEX idx_encounters_provider ON clinical_encounters(provider_id);
CREATE INDEX idx_encounters_date ON clinical_encounters(encounter_date);
CREATE INDEX idx_encounters_type ON clinical_encounters(encounter_type);
CREATE INDEX idx_encounters_status ON clinical_encounters(status);
```

#### Billing Data Model
```sql
CREATE TABLE billing_charges (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    patient_id UUID NOT NULL REFERENCES patients(id),
    encounter_id UUID REFERENCES clinical_encounters(id),
    charge_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    charge_code VARCHAR(20) NOT NULL,
    description TEXT NOT NULL,
    quantity DECIMAL(10,2) DEFAULT 1.00,
    unit_price DECIMAL(10,2) NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    insurance_amount DECIMAL(10,2) DEFAULT 0.00,
    patient_amount DECIMAL(10,2) DEFAULT 0.00,
    status VARCHAR(20) DEFAULT 'pending',
    insurance_claim_id UUID,
    billing_provider_id UUID NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB
);

-- Indexes for billing queries
CREATE INDEX idx_charges_patient ON billing_charges(patient_id);
CREATE INDEX idx_charges_encounter ON billing_charges(encounter_id);
CREATE INDEX idx_charges_date ON billing_charges(charge_date);
CREATE INDEX idx_charges_status ON billing_charges(status);
CREATE INDEX idx_charges_code ON billing_charges(charge_code);
```

### Data Integration Patterns

#### Event-Driven Integration
```yaml
Patient Registration Flow:
  - Event: patient.registered
  - Source: Patient Service
  - Consumers:
    - Clinical Service: Create clinical record
    - Billing Service: Create billing account
    - Appointment Service: Enable scheduling
    - Analytics Service: Update analytics

Clinical Encounter Flow:
  - Event: clinical.encounter.completed
  - Source: Clinical Service
  - Consumers:
    - Billing Service: Generate charges
    - Analytics Service: Update clinical metrics
    - Quality Service: Update quality measures
    - Patient Service: Update patient history
```

#### API Integration Patterns
```typescript
// REST API Pattern
interface PatientAPI {
  createPatient(patientData: Patient): Promise<Patient>;
  getPatient(id: string): Promise<Patient>;
  updatePatient(id: string, patientData: Patient): Promise<Patient>;
  searchPatients(criteria: SearchCriteria): Promise<Patient[]>;
}

// GraphQL Pattern
type Query {
  patient(id: ID!): Patient
  patients(filter: PatientFilter): [Patient!]!
  clinicalEncounters(patientId: ID!): [ClinicalEncounter!]!
}

type Mutation {
  createPatient(input: CreatePatientInput!): Patient!
  updatePatient(id: ID!, input: UpdatePatientInput!): Patient!
  createClinicalEncounter(input: CreateClinicalEncounterInput!): ClinicalEncounter!
}
```

## üîê Security Architecture

### Zero-Trust Security Model

#### Authentication & Authorization
- **Multi-Factor Authentication**: Required for all users
- **OAuth 2.0/OIDC**: Standard authentication protocols
- **JWT Tokens**: Stateless authentication tokens
- **Role-Based Access Control**: Granular permissions
- **Attribute-Based Access Control**: Context-aware permissions

#### Data Security
- **End-to-End Encryption**: All data encrypted in transit and at rest
- **HIPAA-Compliant Encryption**: Fernet encryption for PHI data
- **Data Masking**: Sensitive data masking in logs and displays
- **Audit Logging**: Comprehensive audit trails for all data access
- **Data Retention**: Configurable data retention policies

#### Network Security
- **Service Mesh**: mTLS for service-to-service communication
- **Network Segmentation**: Micro-segmentation by service type
- **API Gateway**: Centralized API management and security
- **Web Application Firewall**: Protection against web attacks
- **DDoS Protection**: Distributed denial of service protection

### Security Implementation

#### Authentication Flow
```mermaid
sequenceDiagram
    participant U as User
    participant C as Client
    participant AG as API Gateway
    participant AS as Auth Service
    participant MS as Microservice

    U->>C: Login Request
    C->>AG: POST /auth/login
    AG->>AS: Forward credentials
    AS->>AS: Validate credentials
    AS->>AS: Generate JWT token
    AS->>AG: Return JWT token
    AG->>C: Return JWT token
    C->>AG: API Request with JWT
    AG->>AG: Validate JWT token
    AG->>MS: Forward request with user context
    MS->>MS: Authorize request
    MS->>AG: Return response
    AG->>C: Return response
```

#### Data Encryption
```python
# Data Encryption Example
from cryptography.fernet import Fernet

class DataEncryption:
    def __init__(self, key: str):
        self.cipher = Fernet(key.encode())

    def encrypt_data(self, data: str) -> str:
        """Encrypt sensitive data"""
        return self.cipher.encrypt(data.encode()).decode()

    def decrypt_data(self, encrypted_data: str) -> str:
        """Decrypt sensitive data"""
        return self.cipher.decrypt(encrypted_data.encode()).decode()

# Usage in models
class Patient(models.Model):
    first_name = models.CharField(max_length=100)
    # Sensitive data encrypted at rest
    ssn_encrypted = models.CharField(max_length=255)

    def set_ssn(self, ssn: str):
        """Encrypt SSN before saving"""
        self.ssn_encrypted = encrypt_data(ssn)

    def get_ssn(self) -> str:
        """Decrypt SSN for authorized access"""
        return decrypt_data(self.ssn_encrypted)
```

## üöÄ Deployment Architecture

### Kubernetes Deployment

#### Cluster Architecture
```yaml
# Kubernetes Cluster Configuration
apiVersion: v1
kind: Namespace
metadata:
  name: hms-enterprise-grade
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: patient-service
  namespace: hms-enterprise-grade
spec:
  replicas: 3
  selector:
    matchLabels:
      app: patient-service
  template:
    metadata:
      labels:
        app: patient-service
    spec:
      containers:
      - name: patient-service
        image: hms-enterprise-grade/patient-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-credentials
              key: url
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

#### Service Mesh Configuration
```yaml
# Istio Service Mesh Configuration
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: hms-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: hms-tls-secret
    hosts:
    - hms-enterprise-grade.com
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: patient-service
spec:
  hosts:
  - hms-enterprise-grade.com
  gateways:
  - hms-gateway
  http:
  - match:
    - uri:
        prefix: /api/patients
    route:
    - destination:
        host: patient-service
        port:
          number: 8000
```

### CI/CD Pipeline

#### GitHub Actions Configuration
```yaml
# .github/workflows/ci-cd.yml
name: HMS CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: hms-enterprise-grade

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run tests
      run: |
        pytest tests/ --cov=. --cov-report=xml
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/hms_test

    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ env.REGISTRY }}/${{ github.repository }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v3

    - name: Set up Kubernetes
      uses: azure/setup-kubectl@v3
      with:
        version: '1.28.0'

    - name: Deploy to Kubernetes
      run: |
        kubectl apply -f k8s/
        kubectl rollout status deployment/patient-service
        kubectl rollout status deployment/clinical-service
```

## üìä Monitoring & Observability

### Monitoring Stack

#### Prometheus Configuration
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true

  - job_name: 'hms-services'
    static_configs:
      - targets: ['patient-service:8000', 'clinical-service:8000', 'billing-service:8000']
```

#### Grafana Dashboards
```json
{
  "dashboard": {
    "title": "HMS System Overview",
    "panels": [
      {
        "title": "API Response Time",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{method}} {{status}}"
          }
        ]
      },
      {
        "title": "Database Connections",
        "type": "gauge",
        "targets": [
          {
            "expr": "pg_stat_database_numbackends",
            "legendFormat": "{{datname}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "5xx Errors"
          }
        ]
      }
    ]
  }
}
```

### Logging Architecture

#### ELK Stack Configuration
```yaml
# Filebeat Configuration
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/hms/*.log
  fields:
    service: hms-enterprise-grade
  fields_under_root: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  indices:
    - index: "hms-logs-%{+yyyy.MM.dd}"

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

#### Log Aggregation
```python
# Centralized Logging Example
import structlog
import logging
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

# Configure tracing
trace.set_tracer_provider(TracerProvider())
jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger",
    agent_port=6831,
)
span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Usage in services
logger = structlog.get_logger()
tracer = trace.get_tracer(__name__)

@tracer.start_as_current_span("process_patient_data")
def process_patient_data(patient_data):
    logger.info("Processing patient data", patient_id=patient_data['id'])

    # Business logic here
    result = process_patient_logic(patient_data)

    logger.info("Patient data processed successfully",
                patient_id=patient_data['id'],
                result=result)

    return result
```

## üîÑ Event-Driven Architecture

### Event Streaming with Kafka

#### Kafka Topics Configuration
```yaml
# Kafka Topics
topics:
  - name: patient-events
    partitions: 6
    replication-factor: 3
    config:
      retention.ms: 604800000  # 7 days
      cleanup.policy: delete

  - name: clinical-events
    partitions: 12
    replication-factor: 3
    config:
      retention.ms: 2592000000  # 30 days
      cleanup.policy: delete

  - name: billing-events
    partitions: 8
    replication-factor: 3
    config:
      retention.ms: 2592000000  # 30 days
      cleanup.policy: delete

  - name: audit-events
    partitions: 6
    replication-factor: 3
    config:
      retention.ms: 31536000000  # 1 year
      cleanup.policy: delete
```

#### Event Producer Example
```python
# Kafka Event Producer
from kafka import KafkaProducer
from kafka.errors import KafkaError
import json
import uuid
from datetime import datetime

class EventProducer:
    def __init__(self, bootstrap_servers):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: str(k).encode('utf-8'),
            acks='all',
            retries=3,
            max_in_flight_requests_per_connection=1,
            enable_idempotence=True
        )

    def publish_event(self, topic, event_type, event_data, event_id=None):
        """Publish event to Kafka topic"""
        if event_id is None:
            event_id = str(uuid.uuid4())

        event = {
            'event_id': event_id,
            'event_type': event_type,
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'version': '1.0',
            'source': 'hms-enterprise-grade',
            'data': event_data
        }

        try:
            future = self.producer.send(topic, key=event_id, value=event)
            record_metadata = future.get(timeout=10)

            logger.info(f"Event published successfully",
                       topic=topic,
                       partition=record_metadata.partition,
                       offset=record_metadata.offset)

            return event_id

        except KafkaError as e:
            logger.error(f"Failed to publish event", error=str(e))
            raise

# Usage example
event_producer = EventProducer(['kafka1:9092', 'kafka2:9092', 'kafka3:9092'])

# Publish patient created event
event_producer.publish_event(
    topic='patient-events',
    event_type='patient.created',
    event_data={
        'patient_id': 'P123456',
        'first_name': 'John',
        'last_name': 'Doe',
        'date_of_birth': '1990-01-15',
        'gender': 'M',
        'created_by': 'user123'
    }
)
```

#### Event Consumer Example
```python
# Kafka Event Consumer
from kafka import KafkaConsumer
from kafka.errors import KafkaError
import json
import logging

logger = logging.getLogger(__name__)

class EventConsumer:
    def __init__(self, bootstrap_servers, group_id):
        self.consumer = KafkaConsumer(
            *['patient-events', 'clinical-events', 'billing-events'],
            bootstrap_servers=bootstrap_servers,
            group_id=group_id,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            key_deserializer=lambda k: k.decode('utf-8'),
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            auto_commit_interval_ms=1000,
            session_timeout_ms=30000,
            heartbeat_interval_ms=10000
        )

    def start_consuming(self, message_handler):
        """Start consuming messages from Kafka topics"""
        logger.info("Starting event consumer")

        try:
            for message in self.consumer:
                try:
                    message_handler(message)
                except Exception as e:
                    logger.error(f"Error processing message",
                               error=str(e),
                               topic=message.topic,
                               partition=message.partition,
                               offset=message.offset)

        except KeyboardInterrupt:
            logger.info("Stopping event consumer")
        finally:
            self.consumer.close()

# Usage example
def handle_patient_event(message):
    """Handle patient-related events"""
    event = message.value
    event_type = event['event_type']

    if event_type == 'patient.created':
        logger.info(f"Processing patient created event",
                   patient_id=event['data']['patient_id'])
        # Process patient creation
        process_patient_creation(event['data'])

    elif event_type == 'patient.updated':
        logger.info(f"Processing patient updated event",
                   patient_id=event['data']['patient_id'])
        # Process patient update
        process_patient_update(event['data'])

# Start consumer
consumer = EventConsumer(['kafka1:9092', 'kafka2:9092', 'kafka3:9092'], 'analytics-service')
consumer.start_consuming(handle_patient_event)
```

## üéØ Performance Optimization

### Caching Strategy

#### Redis Configuration
```yaml
# Redis Configuration
maxmemory 1gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000

# Persistence
appendonly yes
appendfsync everysec

# Security
requirepass your_redis_password

# Networking
bind 127.0.0.1
port 6379
timeout 300
tcp-keepalive 60
```

#### Caching Implementation
```python
# Redis Cache Implementation
import redis
import json
from functools import wraps
from datetime import timedelta

class CacheManager:
    def __init__(self, host='localhost', port=6379, db=0, password=None):
        self.redis_client = redis.Redis(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=True,
            socket_keepalive=True,
            socket_keepalive_options={}
        )

    def get(self, key):
        """Get value from cache"""
        try:
            value = self.redis_client.get(key)
            if value:
                return json.loads(value)
            return None
        except Exception as e:
            logger.error(f"Cache get error", error=str(e))
            return None

    def set(self, key, value, expiration=3600):
        """Set value in cache with expiration"""
        try:
            self.redis_client.setex(
                key,
                expiration,
                json.dumps(value, default=str)
            )
        except Exception as e:
            logger.error(f"Cache set error", error=str(e))

    def delete(self, key):
        """Delete value from cache"""
        try:
            self.redis_client.delete(key)
        except Exception as e:
            logger.error(f"Cache delete error", error=str(e))

    def cache_result(self, expiration=3600):
        """Decorator for caching function results"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # Create cache key from function name and arguments
                cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"

                # Try to get from cache
                result = self.get(cache_key)
                if result is not None:
                    return result

                # Execute function and cache result
                result = func(*args, **kwargs)
                self.set(cache_key, result, expiration)

                return result
            return wrapper
        return decorator

# Usage example
cache_manager = CacheManager(password='your_redis_password')

@cache_manager.cache_result(expiration=1800)  # 30 minutes
def get_patient_data(patient_id):
    """Get patient data with caching"""
    # Database query to get patient data
    patient = Patient.objects.get(id=patient_id)
    return patient.to_dict()
```

### Database Optimization

#### Query Optimization
```python
# Optimized Database Queries
from django.db import models
from django.db.models import Q, Prefetch, Count
from django.core.paginator import Paginator

class PatientQuerySet(models.QuerySet):
    """Custom queryset for Patient model with optimized queries"""

    def with_clinical_data(self):
        """Prefetch related clinical data"""
        return self.prefetch_related(
            Prefetch('clinical_encounters',
                    queryset=ClinicalEncounter.objects.select_related('provider'))
        )

    def with_billing_data(self):
        """Prefetch related billing data"""
        return self.prefetch_related(
            Prefetch('billing_charges',
                    queryset=BillingCharge.objects.select_related('billing_provider'))
        )

    def search_patients(self, query):
        """Optimized patient search"""
        return self.filter(
            Q(first_name__icontains=query) |
            Q(last_name__icontains=query) |
            Q(patient_id__icontains=query) |
            Q(email__icontains=query)
        ).distinct()

    def active_patients(self):
        """Get active patients with optimization"""
        return self.filter(is_active=True).select_related('primary_care Physician')

class Patient(models.Model):
    # Model fields...

    objects = PatientQuerySet.as_manager()

    @classmethod
    def get_patient_list(cls, page=1, per_page=20, search_query=None):
        """Get paginated patient list with optimization"""
        queryset = cls.objects.all()

        if search_query:
            queryset = queryset.search_patients(search_query)

        # Use Paginator for efficient pagination
        paginator = Paginator(queryset, per_page)
        page_obj = paginator.get_page(page)

        return {
            'patients': list(page_obj.object_list),
            'total': paginator.count,
            'pages': paginator.num_pages,
            'current_page': page_obj.number,
            'has_next': page_obj.has_next(),
            'has_previous': page_obj.has_previous()
        }
```

#### Database Indexing Strategy
```sql
-- Composite indexes for common query patterns
CREATE INDEX idx_patient_search ON patients (last_name, first_name, date_of_birth);
CREATE INDEX idx_clinical_encounter_search ON clinical_encounters (patient_id, encounter_date, status);
CREATE INDEX idx_billing_search ON billing_charges (patient_id, charge_date, status);

-- Partial indexes for performance
CREATE INDEX idx_active_patients ON patients (patient_id) WHERE is_active = true;
CREATE INDEX idx_pending_charges ON billing_charges (id) WHERE status = 'pending';

-- Functional indexes for JSON data
CREATE INDEX idx_patient_allergies ON patients USING gin (allergies);
CREATE INDEX idx_clinical_diagnoses ON clinical_encounters USING gin (diagnoses);
```

## üìà Scalability Architecture

### Horizontal Scaling

#### Auto-scaling Configuration
```yaml
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: patient-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: patient-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

#### Load Balancing
```yaml
# Service Load Balancer
apiVersion: v1
kind: Service
metadata:
  name: patient-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  selector:
    app: patient-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  externalTrafficPolicy: Local
```

### Database Scaling

#### Read Replicas
```yaml
# PostgreSQL Read Replica Configuration
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-replica
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_DB
          value: hms_enterprise
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: POSTGRES_REPLICATION_USER
          value: replicator
        - name: POSTGRES_REPLICATION_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: replication_password
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 100Gi
```

#### Connection Pooling
```python
# Database Connection Pooling
from django.db import connections
from django.db.utils import ConnectionHandler
import threading

class DatabaseConnectionPool:
    def __init__(self, max_connections=20):
        self.max_connections = max_connections
        self.connections = {}
        self.lock = threading.Lock()

    def get_connection(self, alias='default'):
        """Get database connection from pool"""
        thread_id = threading.get_ident()

        with self.lock:
            if thread_id not in self.connections:
                if len(self.connections) >= self.max_connections:
                    raise Exception("Maximum database connections reached")

                self.connections[thread_id] = connections[alias]

            return self.connections[thread_id]

    def close_all(self):
        """Close all database connections"""
        with self.lock:
            for connection in self.connections.values():
                connection.close()
            self.connections.clear()

# Configure Django to use connection pooling
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'hms_enterprise',
        'USER': 'postgres',
        'PASSWORD': 'your_password',
        'HOST': 'postgres-primary',
        'PORT': '5432',
        'CONN_MAX_AGE': 0,  # Disable Django's built-in connection pooling
        'OPTIONS': {
            'connect_timeout': 10,
            'application_name': 'hms-enterprise-grade',
        }
    },
    'replica': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'hms_enterprise',
        'USER': 'postgres',
        'PASSWORD': 'your_password',
        'HOST': 'postgres-replica',
        'PORT': '5432',
        'CONN_MAX_AGE': 0,
        'OPTIONS': {
            'connect_timeout': 10,
            'application_name': 'hms-enterprise-grade-read',
        }
    }
}
```

---

This comprehensive architecture documentation provides a detailed technical overview of the HMS Enterprise-Grade system. The architecture is designed to meet the specific needs of healthcare applications while ensuring scalability, security, and performance.

*Last Updated: September 17, 2025*
*Documentation Version: v2.1.0*